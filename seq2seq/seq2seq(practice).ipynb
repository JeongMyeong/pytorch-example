{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170651"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 다운로드 링크 : http://www.manythings.org/anki\n",
    "# Base Code : https://wikidocs.net/24996\n",
    "# 참고 : https://discuss.pytorch.org/t/understanding-lstm-input/31110\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "lines = pd.read_csv('dataset/fra.txt', names=['src', 'tar','drop'], sep='\\t')\n",
    "lines.drop(['drop'], inplace=True, axis=1)\n",
    "len(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4603</th>\n",
       "      <td>You are good.</td>\n",
       "      <td>Vous êtes bon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432</th>\n",
       "      <td>Try it again.</td>\n",
       "      <td>Essaye de nouveau.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9133</th>\n",
       "      <td>Tom loves Mary.</td>\n",
       "      <td>Tom adore Marie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7876</th>\n",
       "      <td>I like talking.</td>\n",
       "      <td>Je me plais à causer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>You're thin.</td>\n",
       "      <td>Vous êtes minces.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>Dig faster.</td>\n",
       "      <td>Creusez plus vite.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8272</th>\n",
       "      <td>I'm not racist.</td>\n",
       "      <td>Je ne suis pas raciste.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>I see Tom.</td>\n",
       "      <td>Je vois Tom.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>Get a doctor.</td>\n",
       "      <td>Va chercher un médecin !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>I've seen it.</td>\n",
       "      <td>Je l'ai vu.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  src                       tar\n",
       "4603    You are good.            Vous êtes bon.\n",
       "4432    Try it again.        Essaye de nouveau.\n",
       "9133  Tom loves Mary.          Tom adore Marie.\n",
       "7876  I like talking.     Je me plais à causer.\n",
       "3114     You're thin.         Vous êtes minces.\n",
       "1121      Dig faster.        Creusez plus vite.\n",
       "8272  I'm not racist.   Je ne suis pas raciste.\n",
       "659        I see Tom.              Je vois Tom.\n",
       "3330    Get a doctor.  Va chercher un médecin !\n",
       "3964    I've seen it.               Je l'ai vu."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[0:10000]\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4625</th>\n",
       "      <td>You go first.</td>\n",
       "      <td>&lt;SOS&gt; Vous en premier. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>It's ours.</td>\n",
       "      <td>&lt;SOS&gt; C'est le nôtre. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>I'll try it.</td>\n",
       "      <td>&lt;SOS&gt; Je le tenterai. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2907</th>\n",
       "      <td>We can't go.</td>\n",
       "      <td>&lt;SOS&gt; Nous ne pouvons pas partir. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8165</th>\n",
       "      <td>I'll risk that.</td>\n",
       "      <td>&lt;SOS&gt; Je tenterai le coup. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  src                                      tar\n",
       "4625    You go first.             <SOS> Vous en premier. <EOS>\n",
       "804        It's ours.              <SOS> C'est le nôtre. <EOS>\n",
       "2395     I'll try it.              <SOS> Je le tenterai. <EOS>\n",
       "2907     We can't go.  <SOS> Nous ne pouvons pas partir. <EOS>\n",
       "8165  I'll risk that.         <SOS> Je tenterai le coup. <EOS>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines['tar'] = lines['tar'].apply(lambda x : '<SOS> ' + x + ' <EOS>')\n",
    "lines.sample(5)\n",
    "# 시작을 의미하는 심볼과 종료를 의미하는 심볼을 각각 \\t 와 \\n으로 표현."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자 집합 구축\n",
    "# src_vocab = set()\n",
    "# for line in lines['src']:\n",
    "#     for char in line:\n",
    "#         src_vocab.add(char)\n",
    "\n",
    "# tar_vocab=set()\n",
    "# for line in lines['tar']:\n",
    "#     for char in line:\n",
    "#         tar_vocab.add(char)\n",
    "\n",
    "# src_vocab = set([a for b in lines['src'] for a in b]) # 위 코드와 결과는 같음\n",
    "# tar_vocab = set([a for b in lines['tar'] for a in b])\n",
    "# 단어 집합이 아니라 글자 집합이라고 하는 이유는 토큰 단위가 단어가 아니라 글자이기 때문\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "src_vocab = set([a for b in lines['src'] for a in b.split(' ')])\n",
    "tar_vocab = set([a for b in lines['tar'] for a in b.split(' ')])\n",
    "# 단어 집합 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3102\n",
      "5999\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$100.', '$5.', '&', '17,', '19.']\n",
      "['!', '$100.', '19', '2:30.', '50']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[0:5])\n",
    "print(tar_vocab[0:5])\n",
    "\n",
    "# 사전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "# print(src_to_index)\n",
    "# print(tar_to_index)\n",
    "# 정수 인코딩을 위한 과정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 정수 인코딩 :  [[164], [190], [190], [316], [316]]\n",
      "프랑스어 정수 인코딩 :  [[10, 1064, 1, 9], [10, 936, 1, 9], [10, 938, 9], [10, 219, 9], [10, 218, 9]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in lines['src']:\n",
    "    temp_X = []\n",
    "    for w in line.split(' '):\n",
    "        temp_X.append(src_to_index[w])\n",
    "    encoder_input.append(temp_X)\n",
    "print('영어 정수 인코딩 : ',encoder_input[:5])\n",
    "\n",
    "\n",
    "decoder_input = []\n",
    "for line in lines['tar']:\n",
    "    temp_X = []\n",
    "    for W in line.split(' '):\n",
    "        temp_X.append(tar_to_index[W])\n",
    "    decoder_input.append(temp_X)\n",
    "print('프랑스어 정수 인코딩 : ', decoder_input[:5])\n",
    "# 정수 인코딩 결과 5개 샘플\n",
    "# 프랑스어는 <sos> 때문에 모두 앞에 1이 붙음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 1064, 1, 9], [10, 936, 1, 9], [10, 938, 9], [10, 219, 9], [10, 218, 9]]\n",
      "[[1064, 1, 9], [936, 1, 9], [938, 9], [219, 9], [218, 9]]\n"
     ]
    }
   ],
   "source": [
    "# decoder_target = []\n",
    "# for line in lines.tar:\n",
    "#     t=0\n",
    "#     temp_X = []\n",
    "#     for w in line:\n",
    "#       if t>0:\n",
    "#         temp_X.append(tar_to_index[w])\n",
    "#       t=t+1\n",
    "#     decoder_target.append(temp_X)\n",
    "# print(decoder_target[:5])\n",
    "print(decoder_input[:5])\n",
    "decoder_target = [k[1:] for k in decoder_input] # 위와 같은 코드\n",
    "print(decoder_target[:5])\n",
    "\n",
    "# 실제값에는 시작 심볼에 해당하는 <sos>가 필요 없으므로 지워준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines['src']])\n",
    "max_tar_len = max([len(line) for line in lines['tar']])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)\n",
    "\n",
    "# 패딩 작업을 위해 가장 긴 길이를 가진 문장의 길이를 구함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')\n",
    "# 영어와 프랑스의 길이는 하나의 쌍이라고 하더라도 전부 다르므로 패딩 할 때도 동일하게 맞춰줄 필요가 없다.\n",
    "# 영어 데이터는 영어 샘플끼리 프랑스어는 프랑스어 끼리 맞추어 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import to_categorical\n",
    "# encoder_input = to_categorical(encoder_input)\n",
    "# decoder_input = to_categorical(decoder_input)\n",
    "# decoder_target = to_categorical(decoder_target)\n",
    "\n",
    "# # 글자단위 번역기므로 워드 임베딩은 별도로 사용 안함.\n",
    "# # 단어 단위로 바꾸어 워드임베딩을 활용한 seq2seq도 시도예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder_tensor = torch.LongTensor(encoder_input[:10000]).to(device)\n",
    "decoder_tensor = torch.LongTensor(decoder_input[:10000]).to(device)\n",
    "decoder_target = torch.LongTensor(decoder_input[:10000]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input  :   torch.Size([10000, 16])\n",
      "decoder_input  :   torch.Size([10000, 69])\n",
      "decoder_target :   torch.Size([10000, 69])\n"
     ]
    }
   ],
   "source": [
    "print('encoder_input  :  ',encoder_tensor.shape)\n",
    "print('decoder_input  :  ',decoder_tensor.shape)\n",
    "print('decoder_target :  ',decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        hidden_size=32\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(src_vocab_size, self.hidden_size)                       # dictionary size, max(sentence length)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, X,h,c):\n",
    "        X = self.embedding(X.view(-1))\n",
    "        X, (hn, cn) = self.lstm(X.view(1,1,-1), (h, c))\n",
    "        return X, (hn, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_hn = torch.zeros(1,1,32).to(device)\n",
    "first_cn = torch.zeros(1,1,32).to(device)\n",
    "enc = Encoder().to(device)\n",
    "a, (hf, cf) = enc(encoder_tensor[0][0], first_hn, first_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = 32\n",
    "        self.embedding = nn.Embedding(tar_vocab_size, self.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, 1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.out = nn.Linear(self.hidden_size, tar_vocab_size)\n",
    "        \n",
    "    def forward(self, X, h, c):\n",
    "        X = self.embedding(X)\n",
    "        X, (hn, cn) = self.lstm(X.view(1,1, -1), (h, c))\n",
    "        X = self.softmax(self.out(X[0]))\n",
    "        return X, (hn, cn)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_tensor[0][0].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = Decoder().to(device)\n",
    "a, (b,c) = dec(decoder_tensor[0][0].view(-1), hf, cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "batch=100\n",
    "random_choice = random.choice(range(batch))\n",
    "random_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 - 10.0%] loss = 5.2897\n",
      "[200 - 20.0%] loss = 0.4445\n",
      "[300 - 30.0%] loss = 0.3731\n",
      "[400 - 40.0%] loss = 0.2715\n",
      "[500 - 50.0%] loss = 0.2085\n",
      "[600 - 60.0%] loss = 0.1864\n",
      "[700 - 70.0%] loss = 0.1823\n",
      "[800 - 80.0%] loss = 0.1568\n",
      "[900 - 90.0%] loss = 0.1472\n",
      "[1000 - 100.0%] loss = 0.1450\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import random\n",
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "learning_rate=0.001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "iterations = 1000\n",
    "loss_total = 0\n",
    "batch=100\n",
    "\n",
    "for iteration in range(1, iterations+1):\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    random_choice = random.choice(range(batch))\n",
    "    \n",
    "    h = torch.zeros([1,1,32]).to(device)\n",
    "    c = torch.zeros([1,1,32]).to(device)\n",
    "    \n",
    "    encoder_length = encoder_tensor.size(1)\n",
    "    decoder_length = decoder_tensor.size(1)\n",
    "    \n",
    "    for token in range(encoder_length):\n",
    "        _, (h, c) = encoder(encoder_tensor[random_choice][token], h, c)\n",
    "    # 한 문장의 encoder 끝\n",
    "        \n",
    "        \n",
    "        \n",
    "    decoder_input = torch.Tensor([[tar_to_index['<SOS>']]]).long().to(device)\n",
    "    for token in range(decoder_length-1):\n",
    "        dec_out, (h, c) = decoder(decoder_input, h, c)\n",
    "        loss += criterion(dec_out, decoder_target[random_choice][token+1].view(-1))\n",
    "        decoder_input = decoder_target[random_choice][token+1].view(-1)\n",
    "    \n",
    "    loss.backward()  \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    loss_iter = loss.item() / decoder_length\n",
    "    loss_total += loss_iter\n",
    "#     print(decoder_target[0][0])\n",
    "#     print(X.argmax())\n",
    "    if iteration % 100 == 0:\n",
    "        loss_avg = loss_total / 100\n",
    "        loss_total = 0\n",
    "        print(\"[{} - {}%] loss = {:05.4f}\".format(iteration, iteration/iterations * 100, loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
