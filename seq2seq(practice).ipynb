{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170651"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 다운로드 링크 : http://www.manythings.org/anki\n",
    "# Base Code : https://wikidocs.net/24996\n",
    "# 참고 : https://discuss.pytorch.org/t/understanding-lstm-input/31110\n",
    "import pandas as pd\n",
    "lines = pd.read_csv('dataset/fra.txt', names=['src', 'tar','drop'], sep='\\t')\n",
    "lines.drop(['drop'], inplace=True, axis=1)\n",
    "len(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>I can swim.</td>\n",
       "      <td>Je sais nager.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14625</th>\n",
       "      <td>I ran out of gas.</td>\n",
       "      <td>Je suis tombé en panne d'essence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>That's OK.</td>\n",
       "      <td>Il n'y a pas de problème.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11075</th>\n",
       "      <td>I was surprised.</td>\n",
       "      <td>Je fus surpris.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34161</th>\n",
       "      <td>I don't need to know.</td>\n",
       "      <td>Je n'ai pas besoin de le savoir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42549</th>\n",
       "      <td>So what're you saying?</td>\n",
       "      <td>Alors qu'es-tu en train de dire ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52249</th>\n",
       "      <td>He is rather optimistic.</td>\n",
       "      <td>Il est plutôt optimiste.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50841</th>\n",
       "      <td>You don't have to come.</td>\n",
       "      <td>Vous n'êtes pas obligée de venir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12230</th>\n",
       "      <td>They're animals.</td>\n",
       "      <td>Ce sont des animaux.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31707</th>\n",
       "      <td>Well? Will you come?</td>\n",
       "      <td>Alors ? Viendrez-vous ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            src                                tar\n",
       "1230                I can swim.                     Je sais nager.\n",
       "14625         I ran out of gas.  Je suis tombé en panne d'essence.\n",
       "924                  That's OK.          Il n'y a pas de problème.\n",
       "11075          I was surprised.                    Je fus surpris.\n",
       "34161     I don't need to know.   Je n'ai pas besoin de le savoir.\n",
       "42549    So what're you saying?  Alors qu'es-tu en train de dire ?\n",
       "52249  He is rather optimistic.           Il est plutôt optimiste.\n",
       "50841   You don't have to come.  Vous n'êtes pas obligée de venir.\n",
       "12230          They're animals.               Ce sont des animaux.\n",
       "31707      Well? Will you come?            Alors ? Viendrez-vous ?"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[0:60000]\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18529</th>\n",
       "      <td>I dream in French.</td>\n",
       "      <td>\\t Je rêve en français. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20725</th>\n",
       "      <td>This is true love.</td>\n",
       "      <td>\\t C'est l'amour vrai. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>I sliced the apple.</td>\n",
       "      <td>\\t J'ai tranché la pomme. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9270</th>\n",
       "      <td>We never voted.</td>\n",
       "      <td>\\t Nous n'avons jamais voté. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18922</th>\n",
       "      <td>I opened a window.</td>\n",
       "      <td>\\t J'ai ouvert la fenêtre. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       src                              tar\n",
       "18529   I dream in French.       \\t Je rêve en français. \\n\n",
       "20725   This is true love.        \\t C'est l'amour vrai. \\n\n",
       "23811  I sliced the apple.     \\t J'ai tranché la pomme. \\n\n",
       "9270       We never voted.  \\t Nous n'avons jamais voté. \\n\n",
       "18922   I opened a window.    \\t J'ai ouvert la fenêtre. \\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines['tar'] = lines['tar'].apply(lambda x : '\\t ' + x + ' \\n')\n",
    "lines.sample(5)\n",
    "# 시작을 의미하는 심볼과 종료를 의미하는 심볼을 각각 \\t 와 \\n으로 표현."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자 집합 구축\n",
    "# src_vocab = set()\n",
    "# for line in lines['src']:\n",
    "#     for char in line:\n",
    "#         src_vocab.add(char)\n",
    "\n",
    "# tar_vocab=set()\n",
    "# for line in lines['tar']:\n",
    "#     for char in line:\n",
    "#         tar_vocab.add(char)\n",
    "\n",
    "src_vocab = set([a for b in lines['src'] for a in b]) # 위 코드와 결과는 같음\n",
    "tar_vocab = set([a for b in lines['tar'] for a in b])\n",
    "\n",
    "# 단어 집합이 아니라 글자 집합이라고 하는 이유는 토큰 단위가 단어가 아니라 글자이기 때문\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])\n",
    "\n",
    "# 글자 집합에 글자 단위로 저장 된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, '’': 77, '€': 78}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, 'С': 100, '\\u2009': 101, '\\u200b': 102, '‘': 103, '’': 104, '\\u202f': 105}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)\n",
    "# 정수 인코딩을 위한 과정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 정수 인코딩 :  [[30, 64, 10], [31, 58, 10], [31, 58, 10], [41, 70, 63, 2], [41, 70, 63, 2]]\n",
      "프랑스어 정수 인코딩 :  [[1, 3, 48, 53, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 14, 3, 2], [1, 3, 29, 67, 73, 70, 71, 105, 4, 3, 2], [1, 3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in lines['src']:\n",
    "    temp_X = []\n",
    "    for W in line:\n",
    "        temp_X.append(src_to_index[W])\n",
    "    encoder_input.append(temp_X)\n",
    "print('영어 정수 인코딩 : ',encoder_input[:5])\n",
    "\n",
    "\n",
    "decoder_input = []\n",
    "for line in lines['tar']:\n",
    "    temp_X = []\n",
    "    for W in line:\n",
    "        temp_X.append(tar_to_index[W])\n",
    "    decoder_input.append(temp_X)\n",
    "print('프랑스어 정수 인코딩 : ', decoder_input[:5])\n",
    "# 정수 인코딩 결과 5개 샘플\n",
    "# 프랑스어는 <sos> 때문에 모두 앞에 1이 붙음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 48, 53, 3, 4, 3, 2],\n",
       " [3, 45, 53, 64, 73, 72, 3, 4, 3, 2],\n",
       " [3, 45, 53, 64, 73, 72, 14, 3, 2],\n",
       " [3, 29, 67, 73, 70, 71, 105, 4, 3, 2],\n",
       " [3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoder_target = []\n",
    "# for line in lines.tar:\n",
    "#     t=0\n",
    "#     temp_X = []\n",
    "#     for w in line:\n",
    "#       if t>0:\n",
    "#         temp_X.append(tar_to_index[w])\n",
    "#       t=t+1\n",
    "#     decoder_target.append(temp_X)\n",
    "# print(decoder_target[:5])\n",
    "\n",
    "decoder_target = [k[1:] for k in decoder_input] # 위와 같은 코드\n",
    "decoder_target[:5]\n",
    "\n",
    "# 실제값에는 시작 심볼에 해당하는 <sos>가 필요 없으므로 1을 지워준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines['src']])\n",
    "max_tar_len = max([len(line) for line in lines['tar']])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)\n",
    "\n",
    "# 패딩 작업을 위해 가장 긴 길이를 가진 문장의 길이를 구함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')\n",
    "# 영어와 프랑스의 길이는 하나의 쌍이라고 하더라도 전부 다르므로 패딩 할 때도 동일하게 맞춰줄 필요가 없다.\n",
    "# 영어 데이터는 영어 샘플끼리 프랑스어는 프랑스어 끼리 맞추어 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "\n",
    "# 글자단위 번역기므로 워드 임베딩은 별도로 사용 안함.\n",
    "# 단어 단위로 바꾸어 워드임베딩을 활용한 seq2seq도 시도예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder_input = torch.LongTensor(encoder_input[:10000]).to(device)\n",
    "decoder_input = torch.LongTensor(decoder_input[:10000]).to(device)\n",
    "decoder_target = torch.LongTensor(decoder_target[:10000]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 25, 79])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 76, 106])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 76, 106])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 25, 79])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(26, 16)\n",
    "        self.lstm = nn.LSTM(16, 16)\n",
    "        self.linear = nn.Linear(632, 8)\n",
    "        self.gru = nn.GRU(16, 16,2)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x = self.embedding(x)\n",
    "        x = self.linear(x.view(2,-1))\n",
    "#         print(x.shape)\n",
    "        x = self.gru(x.view(1,1,-1), h)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(76,16)\n",
    "        self.gru = nn.GRU(16, 16, 2)\n",
    "        self.out = nn.Linear(16, 106)\n",
    "        self.linear = nn.Linear(848, 8)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x = self.embedding(x)\n",
    "        x = self.linear(x.view(2,-1))\n",
    "        x, hidden = self.gru(x.view(1,1,-1), h)\n",
    "        x = self.softmax(self.out(x[0]))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.0838, -5.5578, -4.9453, -5.5748, -5.2558, -4.3325, -4.2342, -3.7796,\n",
       "         -4.9519, -4.4071, -5.5929, -5.0879, -4.9947, -5.1230, -5.5979, -4.6040,\n",
       "         -4.3605, -4.9744, -4.5965, -5.2337, -4.4453, -4.6218, -4.5530, -4.6715,\n",
       "         -4.9833, -4.8089, -5.2214, -4.6931, -5.0656, -3.9251, -3.3829, -5.5055,\n",
       "         -5.0639, -3.7011, -4.3006, -5.2313, -4.9048, -4.6376, -4.4123, -4.4073,\n",
       "         -4.9252, -4.5101, -5.1682, -4.4916, -4.9180, -4.5391, -5.7064, -5.4400,\n",
       "         -5.3979, -5.2050, -5.0735, -3.9831, -5.0178, -4.9550, -4.5609, -4.7451,\n",
       "         -4.3711, -4.5722, -4.5201, -3.9729, -5.3674, -4.5205, -4.7363, -4.2087,\n",
       "         -4.4568, -4.2360, -4.6294, -4.6890, -5.1006, -4.3818, -4.8047, -5.6181,\n",
       "         -5.3187, -4.8214, -4.8933, -4.8147, -3.7917, -4.9250, -4.9842, -4.8344,\n",
       "         -5.0466, -5.0996, -4.1009, -4.8487, -4.8503, -4.8383, -4.8206, -5.4701,\n",
       "         -5.0396, -4.9047, -5.3011, -5.4768, -4.6477, -5.4777, -4.9042, -4.8383,\n",
       "         -4.5329, -4.9407, -5.2099, -4.6989, -4.1254, -3.7655, -3.8814, -4.0373,\n",
       "         -5.5914, -5.0611]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "H = torch.randn(2,1,16).to(device)\n",
    "decoder = Decoder().to(device)\n",
    "decoder(decoder_input[0][0],H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 106]) torch.Size([1, 106])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "multi-target not supported at C:/w/1/s/tmp_conda_3.7_055457/conda/conda-bld/pytorch_1565416617654/work/aten/src\\THCUNN/generic/ClassNLLCriterion.cu:15",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-6b271075fb31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#         decoder_input = decoder_target[dec_input]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1822\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   1823\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1824\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1825\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1826\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: multi-target not supported at C:/w/1/s/tmp_conda_3.7_055457/conda/conda-bld/pytorch_1565416617654/work/aten/src\\THCUNN/generic/ClassNLLCriterion.cu:15"
     ]
    }
   ],
   "source": [
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "encoder_optimizer = optim.RMSprop(encoder.parameters(), lr = 0.01)\n",
    "decoder_optimizer = optim.RMSprop(decoder.parameters(), lr = 0.01)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "epochs=10\n",
    "loss_total=0\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    \n",
    "#     encoder_hidden = torch.zeros([1, 1, 79]).to(device)\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    encoder_hidden = torch.zeros([2, 1, 16]).to(device)\n",
    "    \n",
    "    encoder_length = encoder_input.size(1)\n",
    "    decoder_length = encoder_input.size(1)\n",
    "    \n",
    "    for enc_input in range(encoder_length):\n",
    "        x, encoder_hidden = encoder(encoder_input[epoch][enc_input], encoder_hidden)\n",
    "\n",
    "    for dec_input in range(decoder_length):\n",
    "        decoder_output = decoder(decoder_input[epoch][dec_input], encoder_hidden)\n",
    "        print(decoder_target[epoch][dec_input].view(1,-1).shape, decoder_output.shape)\n",
    "        loss += criterion(decoder_output, decoder_target[epoch][dec_input].view(1,-1))\n",
    "    break\n",
    "#         decoder_input = decoder_target[dec_input]\n",
    "#     loss.backward()\n",
    "#     encoder_optimizer.step()\n",
    "#     decoder_optimizer.step()\n",
    "    \n",
    "#     loss_iter = loss.item() / decoder_length\n",
    "#     loss_total += loss_iter\n",
    "        \n",
    "#     if i % print_every == 0:\n",
    "#         loss_avg = loss_total / print_every\n",
    "#         loss_total = 0\n",
    "#         print(\"[{} - {}%] loss = {:05.4f}\".format(i, i/n_iter * 100, loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "gru = nn.GRU(input_size = 8, hidden_size = 50, num_layers = 2, batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(1, 1, 8)\n",
    "out, hn = gru(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0266, -0.0701,  0.0305, -0.0472,  0.0093, -0.1129,  0.0224,\n",
       "           -0.0507, -0.0683,  0.0489, -0.0713, -0.0239, -0.0886,  0.0294,\n",
       "            0.0022, -0.1088, -0.0650,  0.1388, -0.0304, -0.0134, -0.0108,\n",
       "           -0.0487,  0.0861,  0.0106,  0.0197,  0.0317,  0.0215, -0.0140,\n",
       "           -0.0582, -0.0122, -0.0061, -0.0332,  0.0734, -0.0026,  0.0439,\n",
       "           -0.0010,  0.0230,  0.0389, -0.0838, -0.0278, -0.0857, -0.0370,\n",
       "            0.0023, -0.0450, -0.0105,  0.0245, -0.0498, -0.1098, -0.0059,\n",
       "           -0.0021]]], grad_fn=<TransposeBackward1>),\n",
       " tensor([[[-0.0644, -0.2389, -0.0406, -0.0137, -0.0669,  0.0608, -0.1522,\n",
       "           -0.1155, -0.0742,  0.1120,  0.0899, -0.0191, -0.2450,  0.1627,\n",
       "           -0.0429,  0.0686,  0.1121, -0.0253,  0.0224, -0.0148, -0.1014,\n",
       "           -0.0990, -0.0176, -0.0057,  0.1014, -0.0936,  0.0217, -0.0960,\n",
       "            0.1097,  0.1364,  0.0418,  0.0141,  0.1343, -0.1115,  0.1690,\n",
       "           -0.1486, -0.0833,  0.0828, -0.1309,  0.0491,  0.0579, -0.0196,\n",
       "           -0.0659, -0.1849, -0.0270,  0.0668, -0.1758, -0.0163, -0.1197,\n",
       "           -0.1068]],\n",
       " \n",
       "         [[ 0.0266, -0.0701,  0.0305, -0.0472,  0.0093, -0.1129,  0.0224,\n",
       "           -0.0507, -0.0683,  0.0489, -0.0713, -0.0239, -0.0886,  0.0294,\n",
       "            0.0022, -0.1088, -0.0650,  0.1388, -0.0304, -0.0134, -0.0108,\n",
       "           -0.0487,  0.0861,  0.0106,  0.0197,  0.0317,  0.0215, -0.0140,\n",
       "           -0.0582, -0.0122, -0.0061, -0.0332,  0.0734, -0.0026,  0.0439,\n",
       "           -0.0010,  0.0230,  0.0389, -0.0838, -0.0278, -0.0857, -0.0370,\n",
       "            0.0023, -0.0450, -0.0105,  0.0245, -0.0498, -0.1098, -0.0059,\n",
       "           -0.0021]]], grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
